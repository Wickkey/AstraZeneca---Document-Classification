{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pypandoc\n",
    "import pandas as pd\n",
    "import docx.document\n",
    "import docx.oxml.table\n",
    "import docx.oxml.text.paragraph\n",
    "import docx.table\n",
    "import docx.text.paragraph\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import joblib\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx.oxml.xmlchemy import OxmlElement\n",
    "import re\n",
    "import os\n",
    "import win32com.client as win32\n",
    "from win32com.client import constants\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_docx(path):\n",
    "    \"\"\"\n",
    "    Convert .doc files to .docx files\n",
    "    \"\"\"\n",
    "   # Opening MS Word\n",
    "    word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "    doc = word.Documents.Open(path)\n",
    "    doc.Activate()\n",
    "\n",
    "    # Rename path with.docx\n",
    "    new_file_abs = os.path.abspath(path)\n",
    "    new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "    # Save and Close\n",
    "    word.ActiveDocument.SaveAs(\n",
    "       new_file_abs, FileFormat = constants.wdFormatXMLDocument\n",
    "    )\n",
    "    doc.Close(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_doc(path):\n",
    "    \"\"\"\n",
    "    Convert .docx files to .doc files\n",
    "    \"\"\"\n",
    "   # Opening MS Word\n",
    "    word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "    docx = word.Documents.Open(path)\n",
    "    docx.Activate()\n",
    "\n",
    "    # Rename path with.docx\n",
    "    new_file_abs = os.path.abspath(path)\n",
    "    new_file_abs = re.sub(r'\\.\\w+$', '.doc', new_file_abs)\n",
    "\n",
    "    # Save and Close\n",
    "    word.ActiveDocument.SaveAs(\n",
    "       new_file_abs, FileFormat = constants.wdFormatXMLDocument\n",
    "    )\n",
    "    docx.Close(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_paragraphs(parent, recursive=True):\n",
    "    \"\"\"\n",
    "    Yield each paragraph and table child within *parent*, in document order.\n",
    "    Each returned value is an instance of Paragraph. *parent*\n",
    "    would most commonly be a reference to a main Document object, but\n",
    "    also works for a _Cell object, which itself can contain paragraphs and tables.\n",
    "    Images are inline objects of Paragraphs.\n",
    "    \"\"\"\n",
    "    if isinstance(parent, docx.document.Document):\n",
    "        parent_elm = parent.element.body\n",
    "    elif isinstance(parent, docx.table._Cell):\n",
    "        parent_elm = parent._tc\n",
    "    else:\n",
    "        raise TypeError(repr(type(parent)))\n",
    "\n",
    "    for child in parent_elm.iterchildren():\n",
    "        if isinstance(child, docx.oxml.text.paragraph.CT_P):\n",
    "            yield docx.text.paragraph.Paragraph(child, parent)\n",
    "        elif isinstance(child, docx.oxml.table.CT_Tbl):\n",
    "            if recursive:\n",
    "                table = docx.table.Table(child, parent)\n",
    "                yield table\n",
    "        else:\n",
    "            print(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, drop_section = True):\n",
    "    \"\"\"\n",
    "    Extracts useful information in a text message. Used internally in modifying the document\n",
    "    \"\"\"\n",
    "    df_new=df[['extracted_text','contains_image','contains_table']]\n",
    "    df_new.reset_index(drop=True, inplace=True)\n",
    "    df_new['extracted_text'] = df_new['extracted_text'].str.strip()\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords  \n",
    "    df_new['words'] = df_new['extracted_text'].apply(lambda x: len(x.split()))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df_new['stopwords'] = df_new['extracted_text'].apply(lambda x: len(set(x.split()) & stop_words))\n",
    "    df_new['endswithdot']=df_new['extracted_text'].str.endswith('.')\n",
    "    df_new['endswithdot']=df_new['endswithdot'].astype(int)\n",
    "    df_new['Table']=df_new['extracted_text'].str.contains('Table')\n",
    "    df_new['Table']=df_new['Table'].astype(int)\n",
    "    df_new['contains section']=df_new['extracted_text'].str.contains('Section')\n",
    "    df_new['contains section']=df_new['contains section'].astype(int)\n",
    "    df_new['contains percentage']=df_new['extracted_text'].str.contains('%')\n",
    "    df_new['contains percentage']=df_new['contains percentage'].astype(int)\n",
    "    df_new['Uppercase'] = df_new['extracted_text'].str.findall(r'[A-Z]').str.len()\n",
    "    df_new['Lowercase'] = df_new['extracted_text'].str.findall(r'[a-z]').str.len()\n",
    "    df_new['percentage capital']=(df_new['Uppercase']*100)/(df_new['Lowercase']+df_new['Uppercase'])\n",
    "    if drop_section:\n",
    "        df_new.drop(columns=['Uppercase','Lowercase','contains section'],inplace=True)\n",
    "    else:\n",
    "        df_new.drop(columns=['Uppercase', 'Lowercase'], inplace=True)\n",
    "    import re\n",
    "    def fun_starts_integer(x):\n",
    "        reg = r\"\\d+?\\s.*\"\n",
    "        pat  = re.compile(reg)\n",
    "        if pat.match(x)==None:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def fun_starts_decimal(x):\n",
    "        reg = r\"\\d+\\.\\d+?\\s.*\"\n",
    "        pat  = re.compile(reg)\n",
    "        if pat.match(x)==None:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    df_new['starts_integer']=df_new['extracted_text'].apply(lambda x:fun_starts_integer(x))\n",
    "    df_new['starts_decimal']=df_new['extracted_text'].apply(lambda x:fun_starts_decimal(x))\n",
    "    df_new.fillna(0,inplace=True)\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_paragraph_before(item, text, style=None):\n",
    "    \"\"\"\n",
    "    Return a newly created paragraph, inserted directly before this\n",
    "    item (Table, etc.).\n",
    "    \"\"\"\n",
    "    p = CT_P.add_p_before(item._element)\n",
    "    p2 = Paragraph(p, item._parent)\n",
    "    p2.text = text\n",
    "    p2.style = style\n",
    "    return p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_paragraph_after(item, text, style=None):\n",
    "    \"\"\"\n",
    "    Return a newly created paragraph, inserted directly before this\n",
    "    item (Table, etc.).\n",
    "    Used internally in update_Document function.\n",
    "    \"\"\"\n",
    "    new_p = OxmlElement(\"w:p\")\n",
    "    item._element.addnext(new_p)\n",
    "    new_para = Paragraph(new_p, item._parent)\n",
    "    new_para.text = text\n",
    "    new_para.style = style\n",
    "    return new_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    Deletes the paragraph that is given as reference.\n",
    "    Used Internally in update_Document\n",
    "    \"\"\"\n",
    "    p = paragraph._element\n",
    "    p.getparent().remove(p)\n",
    "    p._p = p._element = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTags = [\"<<START_HEADING>>\",\"<<START_SUBHEADING>>\",\"<<START_SECTION>>\"]\n",
    "endTags = [\"<<END_HEADING>>\",\"<<END_SUBHEADING>>\",\"<<END_SECTION>>\"]\n",
    "clf = joblib.load(open(\"random_forest_vfinal.pkl\", \"rb\")) \n",
    "def update_Document(document2):\n",
    "    \"\"\"\n",
    "    Adds start and endtags to paragraphs in docx file \n",
    "    predicted using trained classifier.\n",
    "    Takes care of merging consecutive sections,\n",
    "    handling tables etc.\n",
    "    \"\"\"\n",
    "    document = copy.deepcopy(document2)\n",
    "    prev_para = None\n",
    "    prev_isTable = False\n",
    "    for curr in tqdm_notebook(iter_paragraphs(document)):\n",
    "        image = 0\n",
    "        table = 0\n",
    "        hyperlink = 0\n",
    "        if isinstance(curr,docx.text.paragraph.Paragraph):\n",
    "            text = curr.text\n",
    "            if 'graphicData' in curr._p.xml:\n",
    "                image = 1 \n",
    "            if curr._p.xpath(\"./w:hyperlink\"):\n",
    "                hyperlink = 1\n",
    "        elif isinstance(curr,docx.table.Table):\n",
    "            table = 1 \n",
    "            text = \"\"\n",
    "            for row in curr.rows:\n",
    "                for cell in row.cells:\n",
    "                    temp = cell.text\n",
    "                    temp = temp.replace('\\n','')\n",
    "                    text+=temp \n",
    "\n",
    "        df = pd.DataFrame({'extracted_text': [text],'contains_image': [image],'contains_table':[table]})\n",
    "        df = feature_extraction(df)\n",
    "        df = df.drop(['extracted_text'],axis=1)\n",
    "        df = df.loc[~(df==0).all(axis=1)]\n",
    "        try: # try and except block will take care of blank lines\n",
    "            if clf.predict(df)[0] ==0 or clf.predict(df)[0] == 1:\n",
    "                if isinstance(curr,docx.text.paragraph.Paragraph) and hyperlink==0 and image==0:\n",
    "                    curr.text = startTags[clf.predict(df)[0]] + curr.text + endTags[clf.predict(df)[0]]\n",
    "                elif isinstance(curr,docx.table.Table) or hyperlink==1 or image==1:\n",
    "                    insert_paragraph_before(curr,startTags[clf.predict(df)[0]])\n",
    "                    insert_paragraph_after(curr,endTags[clf.predict(df)[0]])\n",
    "                prev_para = None\n",
    "                prev_isTable = False\n",
    "\n",
    "            else:\n",
    "                if prev_para == None:\n",
    "                    if isinstance(curr,docx.text.paragraph.Paragraph) and hyperlink==0 and image==0:\n",
    "                        curr.text = startTags[clf.predict(df)[0]] + curr.text\n",
    "                        curr.add_run(endTags[clf.predict(df)[0]])\n",
    "                        prev_para = curr \n",
    "                        prev_isTable = False \n",
    "                    elif isinstance(curr,docx.table.Table) or hyperlink==1 or image==1:\n",
    "                        insert_paragraph_before(curr,startTags[clf.predict(df)[0]])\n",
    "                        prev_para = insert_paragraph_after(curr,endTags[clf.predict(df)[0]])\n",
    "                        prev_isTable = True \n",
    "\n",
    "                elif prev_para:\n",
    "                    if prev_isTable == False:\n",
    "                        if endTags[clf.predict(df)[0]] in prev_para.text:\n",
    "                            inline = prev_para.runs\n",
    "                            for i in range(len(inline)):\n",
    "                                if endTags[clf.predict(df)[0]] in inline[i].text:\n",
    "                                    text_ = inline[i].text.replace(endTags[clf.predict(df)[0]], '')\n",
    "                                    inline[i].text = text_\n",
    "                                    \n",
    "                        \n",
    "                    elif prev_isTable:\n",
    "                        delete_paragraph(prev_para)\n",
    "                        prev_para = None \n",
    "\n",
    "                    if isinstance(curr,docx.text.paragraph.Paragraph) and hyperlink==0 and image==0:\n",
    "                        curr.add_run(endTags[clf.predict(df)[0]])\n",
    "                        prev_para = curr \n",
    "                        prev_isTable = False \n",
    "\n",
    "                    elif isinstance(curr,docx.table.Table) or hyperlink==1 or image==1:\n",
    "                        prev_para = insert_paragraph_after(curr,endTags[clf.predict(df)[0]])\n",
    "                        prev_isTable = True \n",
    "                \n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ = joblib.load(open(\"best_svc_txt.pkl\", \"rb\"))\n",
    "def update_text(location):\n",
    "    startTags = [\"<<START_HEADING>>\",\"<<START_SUBHEADING>>\",\"<<START_SECTION>>\"]\n",
    "    endTags = [\"<<END_HEADING>>\",\"<<END_SUBHEADING>>\",\"<<END_SECTION>>\"]\n",
    "    document = open(location,'r')\n",
    "    filename, file_extension = os.path.splitext(location)\n",
    "    new_doc = open(filename + 'AstraZeneca Submission' + file_extension,'w')\n",
    "    flag = 0\n",
    "    for text in tqdm_notebook(document.readlines()):\n",
    "        if 'Figure' in text:\n",
    "            image = 1 \n",
    "        else:\n",
    "            image = 0\n",
    "        if 'Table' in text:\n",
    "            table = 1 \n",
    "        else:\n",
    "            table = 0 \n",
    "        text_ = text.rstrip()\n",
    "        df = pd.DataFrame({'extracted_text': [text_],'contains_image': [image],'contains_table':[table]})\n",
    "        df = feature_extraction(df)\n",
    "        df = df.drop(['extracted_text'],axis=1)\n",
    "        df = df.loc[~(df==0).all(axis=1)]\n",
    "        try:\n",
    "            if clf_.predict(df)[0] ==0 or clf_.predict(df)[0] == 1:\n",
    "                if flag == 0:\n",
    "                    temp_text = startTags[clf_.predict(df)[0]] + text + endTags[clf_.predict(df)[0]] + '\\n'\n",
    "                    new_doc.write(temp_text)\n",
    "                elif flag ==1:\n",
    "                    flag = 0\n",
    "                    new_doc.write('<<END_SECTION>>\\n')\n",
    "                    temp_text = startTags[clf_.predict(df)[0]] + text + endTags[clf_.predict(df)[0]] + '\\n'\n",
    "                    new_doc.write(temp_text)\n",
    "                    \n",
    "            else:\n",
    "                if flag ==0:\n",
    "                    flag = 1 \n",
    "                    temp_text = startTags[clf_.predict(df)[0]] + text\n",
    "                    new_doc.write(temp_text)\n",
    "                elif flag ==1:\n",
    "                    temp_text = text \n",
    "                    new_doc.write(temp_text)\n",
    "        except Exception as e:\n",
    "            new_doc.write('\\n')\n",
    "        \n",
    "    if flag ==1:\n",
    "        new_doc.write('<<END_SECTION>>')\n",
    "        flag = 0\n",
    "    \n",
    "    new_doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter path to input files\n",
    "# enter full paths (preferably in backslash way)\n",
    "path1 = r'D:\\Sem7-Vacation\\AZ\\submission\\data\\input-word-document.docx'\n",
    "path2 = r'D:\\Sem7-Vacation\\AZ\\submission\\data\\input-word-document_file.doc' \n",
    "path3 = r'D:\\Sem7-Vacation\\AZ\\submission\\data\\input-plain-document.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file(path):\n",
    "    filename, file_extension = os.path.splitext(path) \n",
    "    \n",
    "    if file_extension == '.docx':\n",
    "        input_doc = Document(path)\n",
    "        out = update_Document(input_doc) \n",
    "        out.save(filename+'AstraZeneca_Submission 1.docx')\n",
    "        \n",
    "    elif file_extension == '.doc':\n",
    "        try:\n",
    "            save_as_docx(path)\n",
    "            path_ = path+'x'\n",
    "            input_doc = Document(path_)\n",
    "            out = update_Document(input_doc)\n",
    "            outputfile = filename+ 'AstraZeneca_Submission 2.docx'\n",
    "            out.save(outputfile)\n",
    "            save_as_doc(outputfile)\n",
    "            os.remove(outputfile)\n",
    "            os.remove(path_)\n",
    "        except Exception as e:\n",
    "            print('Failed to open .doc extension. ',e)\n",
    "            \n",
    "    elif file_extension == '.txt':\n",
    "        update_text(path)     \n",
    "    \n",
    "    else:\n",
    "        print('Invalid File Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859f061785784bf9a997d713d12e7629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CT_SectPr '<w:sectPr>' at 0x1e40de75ae8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_file(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a85cc07da74b04a25f1c7cf3fb909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CT_SectPr '<w:sectPr>' at 0x23dd9c96138>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_file(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72784aa58c5b4c20a0c83974c0141e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1098.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_file(path3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
